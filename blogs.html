<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Poppins:wght@100;200;300;400;500;600;700;800;900&display=swap"
      rel="stylesheet"
    />

    <title>Project blog</title>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css"
    />
    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" />

    <!-- Additional CSS Files -->
    <link rel="stylesheet" href="assets/css/fontawesome.css" />
    <link rel="stylesheet" href="assets/css/templatemo-lugx-gaming.css" />
    <link rel="stylesheet" href="assets/css/owl.css" />
    <link rel="stylesheet" href="assets/css/animate.css" />
    <link
      rel="stylesheet"
      href="https://unpkg.com/swiper@7/swiper-bundle.min.css"
    />
  </head>

  <body>
    <!-- ***** Header Area Start ***** -->
    <header class="header-area header-sticky">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <nav class="main-nav">
              <h5 style="color: white; margin-right: 50px">Project Showcase</h5>
              <h1 style="text-align: center">Chat Application ðŸš€ðŸ¤–</h1>
              <!-- ***** NAV BAR Start ***** -->
              <button
                onclick="location.href='index.html#portfolio'"
                style="
                  height: 40px;
                  width: 150px;
                  margin-top: 10px;
                  margin-left: 30px;
                  margin-bottom: 20px;
                  background-color: chocolate;
                  color: white;
                  border-radius: 10px;
                  border: 0px;
                "
              >
                Home Page
              </button>
              <button
                onclick="location.href='#targetSection'"
                style="
                  height: 40px;
                  width: 150px;
                  margin-top: 10px;
                  margin-left: 30px;
                  /* margin-right: 100px; */
                  margin-bottom: 20px;
                  background-color: chocolate;
                  color: white;
                  border-radius: 10px;
                  border: 0px;
                "
              >
                Project Insights
              </button>
              <!-- ***** NAV BAR End ***** -->
            </nav>
          </div>
        </div>
      </div>
    </header>
    <!-- ***** Header Area End ***** -->
    <div class="main-banner" style="margin-left: -10%">
      <div class="container">
        <div class="row">
          <div class="col-lg-6 align-self-center">
            <div class="caption header-text">
              <h3>Project : Custom Conversational AI</h3>
              <br />
              <div class="functions">
                <h4>Introduction</h4>
                <p style="color: black">
                  Welcome to the Custom Multi-Model Support Chat Bot, an
                  advanced AI-powered application that allows you to choose your
                  desired Language Model (LLM) from a variety of options. This
                  chat bot is developed using models hosted on the Hugging Face
                  Model Hub and is designed to provide a seamless and
                  customizable conversational experience based on cutting-edge
                  AI models.
                </p>
                <h4>Tech Stacks</h4>
                <ul class="tech-stack">
                  <li>
                    <h5 style="color: #ff5733; padding-top: 2%">
                      1. Language Models (LLMs)
                    </h5>
                    We leverage state-of-the-art language models hosted on the
                    Hugging Face Model Hub to power our chat bot. You can choose
                    from a variety of pre-trained models, each offering unique
                    capabilities. Explore models from Hugging Face and other
                    leading sources to enhance your conversational experience.
                  </li>
                  <br />
                  <li>
                    <h5 style="color: #33ff57">2. Python</h5>
                    The backend of our Chat Bot is built using Python, a
                    versatile and powerful programming language. Python's
                    extensive ecosystem of libraries and frameworks makes it an
                    ideal choice for developing AI applications.
                  </li>
                  <br />
                  <li>
                    <h5 style="color: #5733ff">3. Streamlit</h5>
                    The user interface of our application is developed using
                    Streamlit, a user-friendly Python library for creating web
                    applications with minimal code. Streamlit allows for quick
                    prototyping and deployment, making it easy for users to
                    interact with the chat bot effortlessly.
                  </li>
                  <br />

                  <li style="padding-bottom: 5%">
                    <h5 style="color: #ff5733">4. LangChain</h5>
                    It is an open-source framework that helps developers build
                    applications using large language models (LLMs). LLMs are
                    deep-learning models that can answer user queries, such as
                    answering questions or creating images from text-based
                    prompts.
                  </li>
                </ul>
              </div>
            </div>
          </div>
          <div class="col-lg-6">
            <div class="right-image">
              <video width="600px" height="400px" controls>
                <source
                  src="https://firebasestorage.googleapis.com/v0/b/newtry-d602d.appspot.com/o/custom-chat-AI.mp4?alt=media&token=c0410157-a71d-4b11-a7d0-1fdd62ba9a9e"
                  type="video/mp4"
                />
                Your browser does not support the video tag.
              </video>
              <div style="margin-top: 30%; width: 600px">
                <ul class="tech-stack">
                  <li>
                    <h5 style="color: #5733ff">5. Hugging Face</h5>
                    It is a platform that gained popularity for its
                    contributions to the natural language processing (NLP)
                    community. It provides a hub for sharing, discovering, and
                    collaborating on pre-trained machine learning models,
                    particularly in the field of NLP. Hugging Face offers a wide
                    range of pre-trained models that can be easily accessed and
                    used for various natural language understanding tasks,
                    including text generation, sentiment analysis, and question
                    answering.
                  </li>
                  <br />
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="features" id="targetSection">
      <div class="container">
        <div class="row">
          <h2 style="text-align: center; margin-bottom: 20px">
            Project Insights
          </h2>
          <div class="col-lg-3 col-md-6">
            <div class="item">
              <h8>Multi-Model</h8>
              <p>
                Multi-Model Support, Choose from a variety of LLMs to tailor the
                chat bot to your preferences. Models : Llama, fastchat-t5-3b and
                google/flan-t5
              </p>
            </div>
          </div>
          <div class="col-lg-3 col-md-6">
            <div class="item">
              <h8>Advanced A.I</h8>
              <p>
                Advanced AI Capabilities, Benefit from the latest advancements
                in natural language processing and understanding. trained on
                more parameters for better respones.
              </p>
            </div>
          </div>
          <div class="col-lg-3 col-md-6">
            <div class="item">
              <h8>BEST SELLERS PIZZA</h8>
              <p>
                User-Friendly Interface, such as Streamlit which provides an
                intuitive interface for seamless interaction. Which enhanced the
                User interaction and feel.
              </p>
            </div>
          </div>
          <div class="col-lg-3 col-md-6">
            <div class="item">
              <h8>WORST SELLERS PIZZA</h8>
              <p>
                Feel free to explore, customize, and contribute to the
                development of our Custom Multi-Model Support Chat Bot. Open
                Source makes it free to use.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- <br> -->
    <!-- ***** Code Display Section Start ***** -->
    <section class="code-display">
      <div class="container">
        <div class="row">
          <div class="col-lg-12">
            <h2 style="margin-top: 90px; text-align: center">Project Code</h2>
            <p>Let Start Code Explaination...</p>
            <br />
            <h5
              style="
                background-color: #33ff57;
                border-radius: 5px;
                text-align: center;
              "
            >
              Code : app.py
            </h5>
            <h7>
              First of all get the all necessary library. we are using streamlit
              for the User Interface (U.I). Streamlit is an open source library
              that work really well with python and LLMs (Large Language
              Models).<br />
              We are using Langchain for the LLM models and also the chain
              functionality which makes it remember the previous conversations.
            </h7>

            <pre>
              <code class="language-python">
import asyncio
import streamlit as st
from langchain.llms import HuggingFaceHub
from langchain.chains import ConversationChain
import os
from langchain.chains.conversation.memory import ConversationBufferMemory
from langchain.chains.conversation.memory import ConversationSummaryBufferMemory
from dotenv import load_dotenv
import os</code></pre>

            <br />
            <br />
            <h7>
              Now the below code is for the UI section, and defining the options
              and sidebars...
            </h7>
            <pre><code class="language-python">

load_dotenv()

st.sidebar.title("Welcome Wanderers", help='This is just a beta model, and is still in progress!!!')
# Add an image to the sidebar
st.sidebar.image("assets/chatbot.jpg") 

st.sidebar.divider()

# Create a sidebar dropdown
selected_option = st.sidebar.selectbox("Select Model:", ["lmsys/fastchat-t5-3b-v1.0", "google/flan-t5-base"])

# Display the selected option below the dropdown
# st.sidebar.write("Model : ", selected_option)

st.sidebar.divider()

max_length = st.sidebar.slider("Max Length", value=132, min_value=32, max_value=250)
temperature = st.sidebar.slider("Temperature", value=0.60, min_value=0.0, max_value=1.0, step=0.05)
</code></pre>

            <br />
            <br />
            <h7
              >Defining and Initialising the HuggingFace api for loading the llm
              model from huggingface.<br />
              memory for Conversation buffer which stores the conversations.</h7
            >
            <pre><code class="language-python">repo_id = selected_option
  llm = HuggingFaceHub(
      huggingfacehub_api_token=os.getenv('HUGGING_FACE_HUB_API_KEY'),
      repo_id=repo_id,
      model_kwargs={
          'temperature': temperature,
          'max_length': max_length,
      }
  )
  
  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=80)
  Conversation_buf = ConversationChain(
      llm=llm,
      memory=memory
      st.divider()
  )</code></pre>

            <br />
            <br />
            <!-- st.markdown("<h1 style='text-align: center;'>Chat Application ðŸš€ðŸ¤–</h1>", unsafe_allow_html=True) -->
            <h7>
              Creating the placeholders for showing the chat response on the U.I
              screen and storing the conversations in a list. Also taking the
              user input using streamlit text_input function. max_chars will our
              maximum word limit for the input.
            </h7>
            <pre><code class="language-python">
default_value = "This is just a small Chat application, and the A.I name is Mr.Zhongli ðŸ¤— This site, \nbuilt by the Me using HuggingFace Models, Its like having a smart machine that \ncompletes your thoughts ðŸ˜€ Get started by typing a custom snippet, check out the \nrepository, or try one of the examples. Have fun!"
st.text(default_value)

st.divider()

# Create a placeholder for the conversation history
conversation_history_placeholder = st.empty()

# Create a list to store the conversation history
conversation_history = []

user_input = st.text_input("Your Query", max_chars=2024)

</code></pre>
            <br /><br />
            <h7>
              Now creating the main function of the model which is the
              prediction part. We divide the code line by line to understand it
              more deeply. below is the explaination.
            </h7>
            <pre><code class="language-python">
if st.button("Predict"):
    # Append user input to the conversation history
    conversation_history.insert(0 ,f"User: {user_input}")
    
    # Await the coroutine to get the actual text
    prediction = asyncio.run(Conversation_buf.acall(inputs=user_input))
    keys_list = list(prediction.items())
    keys = keys_list[2]
    response = keys[1][5:]
    
    # Append model response to the conversation history
    conversation_history.insert(1, f"Mr.Zhongli: {response}")

    # Update the conversation history placeholder
    #conversation_history_placeholder.text_area("Conversation...", "\n".join(conversation_history), height=200)
    st.subheader('_Response_ :blue[here] :sunglasses:')
    st.write(response)
    # st.text(memory.buffer)
              </code>
            </pre>
            <br />
            <br />
            <h7>
              Here, the user's input is added to the beginning (index 0) of the
              conversation_history list. The user_input variable is assumed to
              contain the input provided by the user.
            </h7>
            <pre><code class="language-python">
    # Append user input to the conversation history
    conversation_history.insert(0 ,f"User: {user_input}")
            </code></pre>

            <br />
            <br />
            <h7>
              This line uses the asyncio.run function to run the asynchronous
              coroutine Conversation_buf.acall and obtain the prediction. It
              seems like acall is an asynchronous function that handles model
              prediction based on the inputs parameter, which is set to the
              user's input.
            </h7>
            <pre><code class="language-python">
    # Await the coroutine to get the actual text
    prediction = asyncio.run(Conversation_buf.acall(inputs=user_input))
            </code></pre>

            <br />
            <br />
            <h7>
              This part extracts information from the prediction. It converts
              the prediction into a list of key-value pairs, takes the third
              key-value pair (index 2), and extracts the value (presumably a
              string). It then removes the first five characters from the
              string, assuming those characters are not needed.
            </h7>
            <pre><code class="language-python">
    keys_list = list(prediction.items())
    keys = keys_list[2]
    response = keys[1][5:]
            </code></pre>

            <br />
            <br />
            <h7>
              The model's response is added to the conversation_history list,
              assuming that response contains the model's generated text.
            </h7>
            <pre><code class="language-python">
    # Append model response to the conversation history
    conversation_history.insert(1, f"Mr.Zhongli: {response}")
            </code></pre>

            <br />
            <br />
            <h7>
              These lines use Streamlit functions to display the model's
              response. The response is shown with a subheader and the text is
              displayed using st.write. The :blue[here] and :sunglasses: seem to
              be Streamlit syntax for styling the text.
            </h7>
            <pre><code class="language-python">
    st.subheader('_Response_ :blue[here] :sunglasses:')
    st.write(response)
            </code>
          </pre>

            <h5 style="text-align: center">Thank You</h5>
          </div>
        </div>
      </div>
    </section>
    <!-- ***** Code Display Section End ***** -->

    <footer>
      <div class="container">
        <div
          class="col-lg-12"
          style="
            display: flex;
            justify-content: space-between;
            align-items: center;
          "
        >
          <p>Copyright Â© 2023 Saurabh Singh. All rights reserved.</p>
          <p>
            <a
              rel="nofollow"
              href="https://www.linkedin.com/in/saurabhsingh-"
              target="_blank"
            >
              Contact Me :
              <img
                style="height: 40px; width: 40px"
                src="https://img.icons8.com/color/48/000000/linkedin.png"
                alt="LinkedIn Logo"
              />
            </a>
            <a
              rel="nofollow"
              href="https://github.com/Saurabh7Goku/Multi_Model-Chat-App/blob/main/README.md"
              target="_blank"
            >
              <img
                style="height: 40px; width: 40px; margin-left: 10px"
                src="https://img.icons8.com/color/48/000000/github.png"
                alt="GitHub Logo"
              />
            </a>
          </p>
        </div>
      </div>
    </footer>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>
    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>
    <script src="assets/js/isotope.min.js"></script>
    <script src="assets/js/owl-carousel.js"></script>
    <script src="assets/js/counter.js"></script>
    <script src="assets/js/custom.js"></script>
  </body>
</html>
